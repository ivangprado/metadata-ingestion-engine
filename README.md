# Metadata-Driven Ingestion Engine for Databricks

This project provides a modular, professional-grade data ingestion engine based on metadata, designed for use in Azure Databricks with Git integration.

---

## ğŸ”§ Project Structure

```bash
metadata-ingestion-engine/
â”‚
â”œâ”€â”€ notebooks/                  # Main entry notebooks
â”‚   â”œâ”€â”€ main_ingestion_dispatcher.py
â”‚   â”œâ”€â”€ main_ingestion_asset.py
â”‚   â””â”€â”€ raw_to_datahub.py
â”‚
â”œâ”€â”€ connectors/                # Modular connector functions
â”‚   â”œâ”€â”€ jdbc.py, delta.py, file.py, api.py, olap.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ metadata/                  # Metadata readers from Azure SQL
â”‚   â””â”€â”€ reader.py
â”‚
â”œâ”€â”€ utils/                     # Helpers: SCD2, logging, validation
â”‚   â””â”€â”€ scd.py
â”‚
â”œâ”€â”€ config/                    # Central settings (JDBC, paths)
â”‚   â””â”€â”€ settings.py
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ Usage in Databricks

1. **Clone the repo into Databricks Repos**:
   > Repos > Add Repo > Paste Git URL

2. **Edit `sys.path.append()` in notebooks** to match your Databricks repo path:
   ```python
   sys.path.append("/Workspace/Repos/<your_user>/metadata-ingestion-engine")
   ```

3. **Run Dispatcher**:
   ```python
   dbutils.notebook.run("../notebooks/main_ingestion_dispatcher", 3600, {
       "sourceid": "S_OLAP",
       "max_threads": "4",
       "use_mock": "true"
   })
   ```

4. **Convert raw to DataHub**:
   ```python
   dbutils.notebook.run("../notebooks/raw_to_datahub", 3600, {
       "sourceid": "S_OLAP",
       "assetid": "A001",
       "assetname": "ventas_olap"
   })
   ```

---

## ğŸ“¦ Dependencies

Install in cluster or in notebook (for OLAP XMLA support):
```python
%pip install olap.xmla
```

---

## ğŸ§ª Test OLAP Mock
Run `test_mock_olap.py` in notebooks to validate end-to-end flow without real cube.

---

## ğŸ” Security Note
Use Azure Key Vault or secret scopes in production instead of storing credentials in metadata directly.
